AWSTemplateFormatVersion: '2010-09-09'
Description: SlideRule Cluster

Parameters:

  #
  # Variables
  #
  Version:
    Type: String
  EnvironmentVersion:
    Type: String
  IsPublic:
    Type: String
  Domain:
    Type: String
  Cluster:
    Type: String
  Organization:
    Type: String
  ProjectBucket:
    Type: String
  ProjectFolder:
    Type: String
  ProjectPublicBucket:
    Type: String
  ContainerRegistry:
    Type: String
  NodeCapacity:
    Type: Number
  TTL:
    Type: Number
  ProvisionerLambdaZipFile:
    Type: String

  #
  # Constants
  #
  ManagerIP:
    Type: String
    Default: "10.0.1.3"
  MonitorIP:
    Type: String
    Default: "10.0.1.4"
  IlbIP:
    Type: String
    Default: "10.0.1.5"
  AvailabilityZone:
    Type: String
    Default: "us-west-2d"
  PublicCIDRBlock:
    Type: String
    Default: "0.0.0.0/0"
  VpcCIDRBlock:
    Type: String
    Default: "10.0.0.0/16"

Resources:

  #
  # Network
  #
  SlideruleVPC:
    Type: AWS::EC2::VPC
    Properties:
      CidrBlock: !Ref VpcCIDRBlock
      EnableDnsSupport: true
      EnableDnsHostnames: true
      Tags:
        - Key: Name
          Value: !Sub "${Cluster}-vpc"
  SlideruleSubnet:
    Type: AWS::EC2::Subnet
    Properties:
      VpcId: !Ref SlideruleVPC
      CidrBlock: !Ref VpcCIDRBlock
      MapPublicIpOnLaunch: true
      AvailabilityZone: !Ref AvailabilityZone
      Tags:
        - Key: Name
          Value: !Sub "${Cluster}-subnet"
  SlideruleIGW:
    Type: AWS::EC2::InternetGateway
    Properties:
      Tags:
        - Key: Name
          Value: !Sub "${Cluster}-gateway"
  AttachIGW:
    Type: AWS::EC2::VPCGatewayAttachment
    Properties:
      InternetGatewayId: !Ref SlideruleIGW
      VpcId: !Ref SlideruleVPC
  SlideruleRouteTable:
    Type: AWS::EC2::RouteTable
    Properties:
      VpcId: !Ref SlideruleVPC
      Tags:
        - Key: Name
          Value: !Sub "${Cluster}-route"
  SlideruleRoute:
    Type: AWS::EC2::Route
    Properties:
      RouteTableId: !Ref SlideruleRouteTable
      DestinationCidrBlock: "0.0.0.0/0"
      GatewayId: !Ref SlideruleIGW
  RouteAssoc:
    Type: AWS::EC2::SubnetRouteTableAssociation
    Properties:
      SubnetId: !Ref SlideruleSubnet
      RouteTableId: !Ref SlideruleRouteTable

  #
  # IAM Roles
  #
  ClusterRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - ec2.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly
        - arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore
        - arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy
        - arn:aws:iam::aws:policy/CloudWatchAgentAdminPolicy
      Policies:
        - PolicyName: !Sub "${Cluster}-secrets-access"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - secretsmanager:GetSecretValue
                Resource: !Sub arn:aws:secretsmanager:${AWS::Region}:${AWS::AccountId}:secret:slideruleearth.io/secrets-*
        - PolicyName: !Sub "${Cluster}-s3-access"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action: ["s3:ListBucket"]
                Resource:
                  - arn:aws:s3:::sliderule
                  - arn:aws:s3:::sliderule-public
                  - arn:aws:s3:::pgc-opendata-dems
                  - arn:aws:s3:::prd-tnm
                  - arn:aws:s3:::esa-worldcover
                  - arn:aws:s3:::noaa-ocs-nationalbathymetry-pds
                  - arn:aws:s3:::dataforgood-fb-data
              - Effect: Allow
                Action: ["s3:GetObject"]
                Resource:
                  - arn:aws:s3:::sliderule/*
                  - arn:aws:s3:::sliderule-public/*
                  - arn:aws:s3:::pgc-opendata-dems/*
                  - arn:aws:s3:::prd-tnm/*
                  - arn:aws:s3:::esa-worldcover/*
                  - arn:aws:s3:::noaa-ocs-nationalbathymetry-pds/*
                  - arn:aws:s3:::dataforgood-fb-data/*
              - Effect: Allow
                Action: ["s3:PutObject"]
                Resource:
                  - !Sub "arn:aws:s3:::${ProjectBucket}/${ProjectFolder}/manager/*"
                  - !Sub "arn:aws:s3:::${ProjectPublicBucket}/*"
        - PolicyName: !Sub "${Cluster}-ec2-access"
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - ec2:DescribeInstances
                  - ec2:TerminateInstances
                Resource: "*"
  InstanceProfile:
    Type: AWS::IAM::InstanceProfile
    Properties:
      Roles: [!Ref ClusterRole]

  #
  # Security Group - Monitor
  #
  MonitorSG:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Monitor Security Group
      VpcId: !Ref SlideruleVPC
      SecurityGroupIngress:
        # Loki (from Promtail)
        - IpProtocol: tcp
          FromPort: 3100
          ToPort: 3100
          CidrIp: !Ref VpcCIDRBlock
        # NGINX (from HAProxy)
        - IpProtocol: tcp
          FromPort: 8040
          ToPort: 8040
          CidrIp: !Ref VpcCIDRBlock
      SecurityGroupEgress:
        # ALL
        - IpProtocol: -1
          FromPort: 0
          ToPort: 0
          CidrIp: !Ref PublicCIDRBlock
      Tags:
        - Key: Name
          Value: !Sub "${Cluster}-monitor-sg"
  #
  # Security Group - ILB
  #
  IlbSG:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: ILB Security Group
      VpcId: !Ref SlideruleVPC
      SecurityGroupIngress:
        # HAProxy
        - IpProtocol: tcp
          FromPort: 443
          ToPort: 443
          CidrIp: !Ref PublicCIDRBlock
        # Orchestrator (from HAProxy)
        - IpProtocol: tcp
          FromPort: 8050
          ToPort: 8050
          CidrIp: !Ref VpcCIDRBlock
        # Node Exporter (from Prometheus)
        - IpProtocol: tcp
          FromPort: 9100
          ToPort: 9100
          CidrIp: !Ref VpcCIDRBlock
      SecurityGroupEgress:
        # ALL
        - IpProtocol: -1
          FromPort: 0
          ToPort: 0
          CidrIp: !Ref PublicCIDRBlock
      Tags:
        - Key: Name
          Value: !Sub "${Cluster}-ilb-sg"

  #
  # Security Group - Manager
  #
  ManagerSG:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Manager Security Group
      VpcId: !Ref SlideruleVPC
      SecurityGroupIngress:
        # Manager (from HAProxy)
        - IpProtocol: tcp
          FromPort: 8030
          ToPort: 8030
          CidrIp: !Ref VpcCIDRBlock
      SecurityGroupEgress:
        # ALL
        - IpProtocol: -1
          FromPort: 0
          ToPort: 0
          CidrIp: !Ref PublicCIDRBlock
      Tags:
        - Key: Name
          Value: !Sub "${Cluster}-manager-sg"

  #
  # Security Group - Cluster
  #
  SlideruleSG:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Sliderule app SG
      VpcId: !Ref SlideruleVPC
      SecurityGroupIngress:
        # SlideRule (from HAProxy)
        - IpProtocol: tcp
          FromPort: 9081
          ToPort: 9081
          CidrIp: !Ref VpcCIDRBlock
        # Node Exporter (from Prometheus)
        - IpProtocol: tcp
          FromPort: 9100
          ToPort: 9100
          CidrIp: !Ref VpcCIDRBlock
      SecurityGroupEgress:
        # ALL
        - IpProtocol: -1
          FromPort: 0
          ToPort: 0
          CidrIp: !Ref PublicCIDRBlock
      Tags:
        - Key: Name
          Value: !Sub "${Cluster}-sliderule-sg"

  #
  # EC2 Instance - ILB
  #
  IlbInstance:
    Type: AWS::EC2::Instance
    Properties:
      ImageId: !Sub "{{resolve:ssm:/aws/service/ecs/optimized-ami/amazon-linux-2023/arm64/recommended/image_id}}"
      InstanceType: c8g.large
      AvailabilityZone: !Ref AvailabilityZone
      IamInstanceProfile: !Ref InstanceProfile
      SourceDestCheck: true
      Monitoring: false
      NetworkInterfaces:
        - DeviceIndex: 0
          PrivateIpAddress: !Ref IlbIP
          AssociatePublicIpAddress: true
          SubnetId: !Ref SlideruleSubnet
          GroupSet:
            - !Ref IlbSG
      BlockDeviceMappings:
        - DeviceName: /dev/xvda
          Ebs:
            VolumeSize: 40
            VolumeType: gp3
            DeleteOnTermination: true
      Tags:
        - Key: Name
          Value: !Sub "${Cluster}-ilb"
      UserData:
        Fn::Base64: !Sub |
          #!/bin/bash
          aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin ${ContainerRegistry}
          SECRET=$(aws secretsmanager get-secret-value --region us-west-2 --secret-id ${Domain}/secrets --query SecretString --output text)
          export OAUTH_HMAC_SECRET=$(echo $SECRET | jq -r .jwt_secret_key)
          mkdir -p /etc/ssl/private
          aws s3 cp s3://${ProjectBucket}/${ProjectFolder}/${Domain}.pem /etc/ssl/private/${Domain}.pem
          aws s3 cp s3://${ProjectBucket}/${ProjectFolder}/docker-compose-linux-aarch64 ./docker-compose
          chmod +x docker-compose
          cat > docker-compose.yml << EOF
          version: "3.9"
          services:
            ilb:
              image: ${ContainerRegistry}/ilb:${Version}
              network_mode: host
              restart: always
              pull_policy: if_not_present
              volumes:
                - /haproxy:/haproxy
                - /etc/ssl/private:/etc/ssl/private
              environment:
                - IS_PUBLIC=${IsPublic}
                - ORGANIZATION=${Organization}
                - DOMAIN=${Domain}
                - OAUTH_HMAC_SECRET=$OAUTH_HMAC_SECRET
              healthcheck:
                test: curl -f http://localhost:8050/discovery/health
                interval: 30s
                timeout: 10s
                retries: 1
                start_period: 30s
              labels:
                - autoheal=true
            monitor-agent:
              image: ${ContainerRegistry}/monitor-agent:${Version}
              network_mode: host
              restart: unless-stopped
              pull_policy: if_not_present
              volumes:
                - /proc:/host/proc:ro
                - /sys:/host/sys:ro
                - /:/rootfs:ro
                - /var/log:/var/log:ro
                - /var/log/journal:/var/log/journal:ro
                - /run/log/journal:/run/log/journal:ro
                - /etc/machine-id:/etc/machine-id:ro
                - /var/lib/docker/containers:/var/lib/docker/containers:ro
              labels:
                - autoheal=true
            autoheal:
              image: willfarrell/autoheal:latest
              container_name: autoheal
              restart: always
              environment:
                - AUTOHEAL_CONTAINER_LABEL=all
                - AUTOHEAL_INTERVAL=30
              volumes:
                - /var/run/docker.sock:/var/run/docker.sock
          EOF
          ./docker-compose -p cluster up --detach

  #
  # EC2 Instance -  Manager
  #
  ManagerInstance:
    Type: AWS::EC2::Instance
    Properties:
      ImageId: !Sub "{{resolve:ssm:/aws/service/ecs/optimized-ami/amazon-linux-2023/arm64/recommended/image_id}}"
      InstanceType: r8g.large
      AvailabilityZone: !Ref AvailabilityZone
      IamInstanceProfile: !Ref InstanceProfile
      SourceDestCheck: true
      Monitoring: false
      NetworkInterfaces:
        - DeviceIndex: 0
          PrivateIpAddress: !Ref ManagerIP
          AssociatePublicIpAddress: true
          SubnetId: !Ref SlideruleSubnet
          GroupSet:
            - !Ref ManagerSG
      BlockDeviceMappings:
        - DeviceName: /dev/xvda
          Ebs:
            VolumeSize: 40
            VolumeType: gp3
            DeleteOnTermination: true
      Tags:
        - Key: Name
          Value: !Sub "${Cluster}-manager"
      UserData:
        Fn::Base64: !Sub |
          #!/bin/bash
          echo ${Cluster} > ./Cluster.txt
          aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin ${ContainerRegistry}
          SECRET=$(aws secretsmanager get-secret-value --region us-west-2 --secret-id ${Domain}/secrets --query SecretString --output text)
          export MANAGER_SECRET_SALT=$(echo $SECRET | jq -r .manager_secret_salt)
          export MANAGER_API_KEY=$(echo $SECRET | jq -r .manager_api_key)
          mkdir /data
          aws s3 cp $DUCKDB_REMOTE_FILE $DUCKDB_LOCAL_FILE || true
          aws s3 cp s3://${ProjectBucket}/${ProjectFolder}/GeoLite2-ASN.mmdb /data/GeoLite2-ASN.mmdb
          aws s3 cp s3://${ProjectBucket}/${ProjectFolder}/GeoLite2-City.mmdb /data/GeoLite2-City.mmdb
          aws s3 cp s3://${ProjectBucket}/${ProjectFolder}/GeoLite2-Country.mmdb /data/GeoLite2-Country.mmdb
          aws s3 cp s3://${ProjectBucket}/${ProjectFolder}/docker-compose-linux-aarch64 ./docker-compose
          chmod +x docker-compose
          cat > docker-compose.yml << EOF
          version: "3.9"
          services:
            manager:
              image: ${ContainerRegistry}/manager:${Version}
              network_mode: host
              restart: always
              pull_policy: if_not_present
              entrypoint: /docker-entrypoint.sh
              volumes:
                - /data:/data
              environment:
                - IS_PUBLIC=${IsPublic}
                - MANAGER_SECRET_SALT=$MANAGER_SECRET_SALT
                - MANAGER_API_KEY=$MANAGER_API_KEY
                - DUCKDB_LOCAL_FILE='/data/manager-${Cluster}.db'
                - DUCKDB_REMOTE_FILE='s3://${ProjectBucket}/${ProjectFolder}/manager/manager-${Cluster}-${Version}.db'
                - ORCHESTRATOR=http://10.0.1.5:8050
              labels:
                - autoheal=true
            monitor-agent:
              image: ${ContainerRegistry}/monitor-agent:${Version}
              network_mode: host
              restart: unless-stopped
              pull_policy: if_not_present
              volumes:
                - /proc:/host/proc:ro
                - /sys:/host/sys:ro
                - /:/rootfs:ro
                - /var/log:/var/log:ro
                - /var/log/journal:/var/log/journal:ro
                - /run/log/journal:/run/log/journal:ro
                - /etc/machine-id:/etc/machine-id:ro
                - /var/lib/docker/containers:/var/lib/docker/containers:ro
              labels:
                - autoheal=true
            autoheal:
              image: willfarrell/autoheal:latest
              container_name: autoheal
              restart: always
              environment:
                - AUTOHEAL_CONTAINER_LABEL=all
                - AUTOHEAL_INTERVAL=30
              volumes:
                - /var/run/docker.sock:/var/run/docker.sock
          EOF
          ./docker-compose -p cluster up --detach

  #
  # EC2 Instance - Monitor
  #
  MonitorInstance:
    Type: AWS::EC2::Instance
    Properties:
      ImageId: !Sub "{{resolve:ssm:/aws/service/ecs/optimized-ami/amazon-linux-2023/arm64/recommended/image_id}}"
      InstanceType: c8g.large
      AvailabilityZone: !Ref AvailabilityZone
      IamInstanceProfile: !Ref InstanceProfile
      SourceDestCheck: true
      Monitoring: false
      NetworkInterfaces:
        - DeviceIndex: 0
          PrivateIpAddress: !Ref MonitorIP
          AssociatePublicIpAddress: true
          SubnetId: !Ref SlideruleSubnet
          GroupSet:
            - !Ref MonitorSG
      BlockDeviceMappings:
        - DeviceName: /dev/xvda
          Ebs:
            VolumeSize: 40
            VolumeType: gp3
            DeleteOnTermination: true
      Tags:
        - Key: Name
          Value: !Sub "${Cluster}-monitor"
      UserData:
        Fn::Base64: !Sub |
          #!/bin/bash
          echo ${Cluster} > ./Cluster.txt
          aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin ${ContainerRegistry}
          SECRET=$(aws secretsmanager get-secret-value --region us-west-2 --secret-id ${Domain}/secrets --query SecretString --output text)
          export CLIENT_ID=$(echo $SECRET | jq -r .client_id)
          export CLIENT_SECRET=$(echo $SECRET | jq -r .client_secret)
          aws s3 cp s3://${ProjectBucket}/${ProjectFolder}/docker-compose-linux-aarch64 ./docker-compose
          chmod +x docker-compose
          cat > docker-compose.yml << EOF
          version: "3.9"
          services:
            monitor:
              image: ${ContainerRegistry}/monitor:${Version}
              network_mode: host
              restart: always
              pull_policy: if_not_present
              labels:
                - autoheal=true
            monitor-agent:
              image: ${ContainerRegistry}/proxy:${Version}
              network_mode: host
              restart: unless-stopped
              pull_policy: if_not_present
              volumes:
                - /proc:/host/proc:ro
                - /sys:/host/sys:ro
                - /:/rootfs:ro
                - /var/log:/var/log:ro
                - /var/log/journal:/var/log/journal:ro
                - /run/log/journal:/run/log/journal:ro
                - /etc/machine-id:/etc/machine-id:ro
                - /var/lib/docker/containers:/var/lib/docker/containers:ro
              labels:
                - autoheal=true
            proxy:
              image: ${ContainerRegistry}/monitor-agent:${Version}
              network_mode: host
              restart: always
              pull_policy: if_not_present
              entrypoint: /usr/local/etc/docker-entrypoint.sh
              volumes:
                - /etc/ssl/private:/etc/ssl/private
              environment:
                - CLIENT_ID=$CLIENT_ID
                - CLIENT_SECRET=$CLIENT_SECRET
                - DOMAIN=${Domain}
                - REDIRECT_URI_SCHEME=https
              labels:
                - autoheal=true
            autoheal:
              image: willfarrell/autoheal:latest
              container_name: autoheal
              restart: always
              environment:
                - AUTOHEAL_CONTAINER_LABEL=all
                - AUTOHEAL_INTERVAL=30
              volumes:
                - /var/run/docker.sock:/var/run/docker.sock
          EOF
          ./docker-compose -p cluster up --detach

  #
  # Autoscaling Group - Cluster
  #
  SlideruleLaunchTemplate:
    Type: AWS::EC2::LaunchTemplate
    Properties:
      LaunchTemplateName: !Sub "${Cluster}-lt"
      LaunchTemplateData:
        ImageId: !Sub "{{resolve:ssm:/aws/service/ecs/optimized-ami/amazon-linux-2023/arm64/recommended/image_id}}"
        InstanceType: t4g.2xlarge
        IamInstanceProfile:
          Name: !Ref InstanceProfile
        NetworkInterfaces:
          - AssociatePublicIpAddress: true
            DeviceIndex: 0
            Groups:
              - !Ref SlideruleSG
        BlockDeviceMappings:
          - DeviceName: "/dev/xvda"
            Ebs:
              VolumeSize: 100
              VolumeType: gp3
              DeleteOnTermination: true
        UserData:
          Fn::Base64: !Sub |
            #!/bin/bash
            export IPV4=$(hostname -I | awk '{print $1}')
            aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin ${ContainerRegistry}
            mkdir -p /data/ATL13
            aws s3 cp s3://${ProjectBucket}/${ProjectFolder}/atl13.json /data/ATL13/atl13.json
            aws s3 cp s3://${ProjectBucket}/${ProjectFolder}/atl13.db /data/ATL13/atl13.db
            mkdir -p /data/ATL24
            aws s3 cp s3://${ProjectBucket}/${ProjectFolder}/atl24r2.db /data/ATL24/atl24r2.db
            mkdir -p /data/3DEP
            aws s3 cp s3://${ProjectBucket}/${ProjectFolder}/3dep.db /data/3DEP/3dep.db
            mkdir -p /plugins
            aws s3 cp s3://${ProjectBucket}/plugins/ /plugins/ --recursive
            aws s3 cp s3://${ProjectBucket}/${ProjectFolder}/docker-compose-linux-aarch64 ./docker-compose
            chmod +x docker-compose
            cat > docker-compose.yml << EOF
            version: "3.9"
            services:
              sliderule:
                image: ${ContainerRegistry}/sliderule:${Version}
                network_mode: host
                restart: always
                pull_policy: if_not_present
                ulimits:
                  nofile:
                    soft: 8192
                    hard: 8192
                volumes:
                  - /etc/ssl/certs:/etc/ssl/certs
                  - /var/run/docker.sock:/var/run/docker.sock
                  - /data:/data
                  - /plugins:/plugins
                environment:
                  - IPV4=$IPV4
                  - ORCHESTRATOR=http://10.0.1.5:8050
                  - MANAGER=http://10.0.1.3:8030
                  - AMS=http://127.0.0.1:9082
                  - ORGANIZATION=${Organization}
                  - CLUSTER=${Cluster}
                  - IS_PUBLIC=${IsPublic}
                  - PROVISIONING_SYSTEM="https://ps.${Domain}"
                  - CONTAINER_REGISTRY=${ContainerRegistry}
                  - ENVIRONMENT_VERSION=${EnvironmentVersion}
                labels:
                  - autoheal=true
                healthcheck:
                  test: curl -f -X GET -d "{}" http://localhost:9081/source/health
                  interval: 30s
                  timeout: 10s
                  retries: 1
                  start_period: 30s
              ams:
                image: ${ContainerRegistry}/ams:${Version}
                network_mode: host
                restart: always
                pull_policy: if_not_present
                entrypoint: /docker-entrypoint.sh
                volumes:
                  - /data:/data
                labels:
                  - autoheal=true
              monitor-agent:
                image: ${ContainerRegistry}/monitor-agent:${Version}
                network_mode: host
                restart: unless-stopped
                pull_policy: if_not_present
                volumes:
                  - /proc:/host/proc:ro
                  - /sys:/host/sys:ro
                  - /:/rootfs:ro
                  - /var/log:/var/log:ro
                  - /var/log/journal:/var/log/journal:ro
                  - /run/log/journal:/run/log/journal:ro
                  - /etc/machine-id:/etc/machine-id:ro
                  - /var/lib/docker/containers:/var/lib/docker/containers:ro
                labels:
                  - autoheal=true
              autoheal:
                image: willfarrell/autoheal:latest
                container_name: autoheal
                restart: always
                environment:
                  - AUTOHEAL_CONTAINER_LABEL=all
                  - AUTOHEAL_INTERVAL=30
                volumes:
                  - /var/run/docker.sock:/var/run/docker.sock
            EOF
            ./docker-compose -p cluster up --detach

  SlideruleAutoScalingGroup:
    Type: AWS::AutoScaling::AutoScalingGroup
    Properties:
      AutoScalingGroupName: !Sub "${Cluster}-asg"
      MinSize: !Ref NodeCapacity
      MaxSize: !Ref NodeCapacity
      DesiredCapacity: !Ref NodeCapacity
      VPCZoneIdentifier:
        - !Ref SlideruleSubnet
      HealthCheckType: EC2
      CapacityRebalance: true
      LaunchTemplate:
        LaunchTemplateId: !Ref SlideruleLaunchTemplate
        Version: !GetAtt SlideruleLaunchTemplate.LatestVersionNumber
      Tags:
        - Key: Name
          Value: !Sub "${Cluster}-node"
          PropagateAtLaunch: true

  #
  # DNS
  #
  IlbRecord:
    Type: AWS::Route53::RecordSet
    Properties:
      HostedZoneName: !Sub "${Domain}."   # ensure trailing dot or you can pass HostedZoneId instead
      Name: !Sub "${Cluster}.${Domain}"
      Type: A
      TTL: "300"
      ResourceRecords:
        - !GetAtt IlbInstance.PublicIp

  #
  # Auto-Shutdown
  #
  DestroyLambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
        - arn:aws:iam::aws:policy/PowerUserAccess  # broad permissions for deletion
  DestroyLambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub '${AWS::StackName}-auto-shutdown'
      Runtime: python3.13
      Handler: main.lambda_destroy
      Role: !GetAtt DestroyLambdaRole.Arn
      Timeout: 60
      Code:
        S3Bucket: !Ref ProjectBucket
        S3Key: !Ref ProvisionerLambdaZipFile
  SchedulerLambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: SchedulerPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - events:PutRule
                  - events:PutTargets
                  - events:DescribeRule
                Resource: '*'
              - Effect: Allow
                Action:
                  - lambda:AddPermission
                Resource: '*'
  SchedulerLambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub '${AWS::StackName}-scheduler'
      Runtime: python3.13
      Handler: main.lambda_scheduler
      Role: !GetAtt SchedulerLambdaRole.Arn
      Timeout: 60
      Code:
        S3Bucket: !Ref ProjectBucket
        S3Key: !Ref ProvisionerLambdaZipFile
  AutoShutdownCustomResource:
    Type: Custom::ScheduleStackDeletion
    DependsOn:
      - SlideruleAutoScalingGroup
    Properties:
      ServiceToken: !GetAtt SchedulerLambdaFunction.Arn
      StackName: !Ref AWS::StackName
      DeleteAfterMinutes: !Ref TTL
      DeletionLambdaArn: !GetAtt DestroyLambdaFunction.Arn
